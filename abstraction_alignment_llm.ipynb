{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcdaffde-cc48-4625-9ff5-e7ae2631cbd1",
   "metadata": {},
   "source": [
    "# Abstraction Alignment to Benchmark Language Models\n",
    "We use abstraction alignment to benchmark language models' specificity. Here we expand specificity benchmarks from the [S-TEST dataset](https://github.com/jeffhj/S-TEST) to include additional hypotheses. This example follows the Quantitatively Comparing Model Specificity case study from the Abstraction Alignment paper (Section 5.2.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0da3f9-862a-4a2e-9737-7b35da57f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdbc3d5-fdf5-4ed7-bbf0-f7f7aa942118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bafcd3-6f7c-401d-a786-5e8c22ceef82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the S-TEST Specificity Testing Benchmaks\n",
    "The S-TEST dataset contains sentences that test the model on a subject's occupation, location, or place of brith. For instance occupation sentences are in the format \"Cher is a [MASK] by profession.\" Each sentence has a corresponding specific label (e.g., \"singer\") and general label (e.g., \"artist\"). Here we load the S-TEST data as well as the prediction results for 5 BERT, RoBERTa, and GPT-2 models.\n",
    "\n",
    "In order to get results, first run `python S-TEST/scripts/run_experiments.py`. See the `README.md` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3274de5f-ac44-4c5a-a16b-e934fb704366",
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE_STUDY_DIR = 'util/llm/'\n",
    "DATA_DIR = 'util/llm/S-TEST/data/S-TEST/'\n",
    "RESULTS_DIR = 'util/llm/S-TEST/output/results/'\n",
    "\n",
    "MODELS = [\n",
    "    'bert_base', \n",
    "    'bert_large', \n",
    "    'roberta.base', \n",
    "    'roberta.large', \n",
    "    'gpt2',\n",
    "]\n",
    "TASKS = [\n",
    "    {'name': 'occupation', 'id': 'P106', 'up_fn': 'hypernyms', 'down_fn': 'hyponyms', 'root': wn.synset('person.n.01')},\n",
    "    {'name': 'location', 'id': 'P131', 'up_fn': 'part_holonyms', 'down_fn': 'part_meronyms', 'root': None},\n",
    "    {'name': 'place of birth', 'id': 'P19', 'up_fn': 'part_holonyms', 'down_fn': 'part_meronyms', 'root': None},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b534ce-197e-4ac9-acda-cde1f1221b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 instances for occupation prediciton task.\n",
      "Example data:\n",
      "{'sub_uri': 'Q39074561', 'sub_label': 'Joe Carter', 'obj_uri': 'Q1371925', 'obj_label': 'announcer', 'obj_value': 2.0, 'obj2_uri': 'Q1930187', 'obj2_label': 'journalist', 'obj2_value': 3.0, 'predicate_id': 'P106'}\n"
     ]
    }
   ],
   "source": [
    "def load_data(task_id):\n",
    "    \"\"\"Load the data instances for a task_id. There can be duplicates, but we\n",
    "    handle them the same way the S-TEST repo does.\"\"\"\n",
    "    data = {}\n",
    "    with open(os.path.join(DATA_DIR, f'{task_id}.jsonl'), 'r') as f:\n",
    "        for line in f:\n",
    "            datum = json.loads(line)\n",
    "            data[datum['sub_label']] = datum\n",
    "    return data\n",
    "\n",
    "task = TASKS[0]\n",
    "model = MODELS[0]\n",
    "\n",
    "data = load_data(task['id'])\n",
    "print(f\"{len(data)} instances for {task['name']} prediciton task.\")\n",
    "print(f\"Example data:\")\n",
    "print(data[list(data.keys())[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7be51d-c8d6-46b5-ab9b-4ce04ec7a592",
   "metadata": {},
   "source": [
    "## Compute Model Accuracy and the S-TEST Specificity Metric\n",
    "The S-TEST specificity metric `p_r` tests how often the model prefers the specific label to the more general label. In our nominclature, we write `p_r = P(s_s, s_g)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c1fb9b-3e79-4bed-a257-5a0e5482b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 predictions for bert_base on occupation prediciton task.\n",
      "Predictions for results[0] sum to 0.8970748439562531\n",
      "Computed probabilities for 18173 words.\n"
     ]
    }
   ],
   "source": [
    "def load_model_results(model_name, task_id):\n",
    "    with open(os.path.join(RESULTS_DIR, model_name, task_id, 'result.pkl'), 'rb') as f:\n",
    "        results = pickle.load(f)['list_of_results']\n",
    "    return results\n",
    "\n",
    "results = load_model_results(model, task['id'])\n",
    "print(f\"{len(results)} predictions for {model} on {task['name']} prediciton task.\")\n",
    "print(f\"Predictions for results[0] sum to {np.sum([np.exp(w['log_prob']) for w in results[0]['masked_topk']['topk']])}\")\n",
    "print(f\"Computed probabilities for {len(results[0]['masked_topk']['topk'])} words.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ecde337-5021-4cc8-80da-18f3986e74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pr(results, data):\n",
    "    specific = 0\n",
    "    for result in results:\n",
    "        subject = result['sample']['sub_label']\n",
    "        data_instance = data[subject]\n",
    "        specific_label = data_instance['obj_label']\n",
    "        coarse_label = data_instance['obj2_label']\n",
    "        for token in result['masked_topk']['topk']:\n",
    "            if token['token_word_form'] == specific_label:\n",
    "                specific += 1\n",
    "                break\n",
    "            if token['token_word_form'] == coarse_label:\n",
    "                break\n",
    "    return specific / len(results)\n",
    "\n",
    "def compute_accuracy(results, data, k=1):\n",
    "    correct = 0\n",
    "    for result in results:\n",
    "        subject = result['sample']['sub_label']\n",
    "        data_instance = data[subject]\n",
    "        label = data_instance['obj_label']\n",
    "        topk_words = [t['token_word_form'] for t in result['masked_topk']['topk'][:k]]\n",
    "        if label in topk_words:\n",
    "            correct += 1\n",
    "    return correct / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b2906d-f3ad-4a01-b267-b574ce01581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base occupation prediction acc@1 = 0.32%; acc@10 = 28.44%\n"
     ]
    }
   ],
   "source": [
    "accuracy_1 = compute_accuracy(results, data, k=1)\n",
    "accuracy_10 = compute_accuracy(results, data, k=10)\n",
    "print(f\"{model} {task['name']} prediction acc@1 = {accuracy_1:.2%}; acc@10 = {accuracy_10:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "296ee1ac-defc-4ba7-a25b-6f476ceeaca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base occupation prediction pr = 70.46%\n"
     ]
    }
   ],
   "source": [
    "pr = compute_pr(results, data)\n",
    "print(f\"{model} {task['name']} prediction pr = {pr:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b101cbe-7c2e-4ccd-875a-34168c553c19",
   "metadata": {},
   "source": [
    "## Compute Abstraction Alignment Specificity Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e15a8-e272-4560-b874-172dcfb1d721",
   "metadata": {},
   "source": [
    "### Get all related words from WordNet\n",
    "Instead of testing two words, with abstraction alignment we can test many concepts over many levels of abstraction. We use WordNet as the human abstraction graph, taking all the concepts that are related to the dataset's labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8772348e-dc9b-4736-bbbc-5e4ac9caa542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_words(synset, traversal_fn_name, root=None, include_self=True):\n",
    "    \"\"\"Returns all words related to synset via the traversal_fn_name.\"\"\"\n",
    "    traversal_fn = getattr(synset, traversal_fn_name)\n",
    "    words = set([])\n",
    "    if include_self:\n",
    "        words.add(synset.name().split('.')[0])\n",
    "    if (root is not None and synset == root) or len(traversal_fn()) == 0:\n",
    "        return words\n",
    "    for word in traversal_fn():\n",
    "        next_words = get_related_words(word, traversal_fn_name, root)\n",
    "        words.update(next_words)\n",
    "    return words\n",
    "\n",
    "def get_abstraction_graph(task):\n",
    "    \"\"\"Creates an abstraction graph with all task-related words.\"\"\"\n",
    "    with open(os.path.join(CASE_STUDY_DIR, f\"{task['id']}_synsets.json\"), 'r') as f:\n",
    "        label_synsets = json.load(f)\n",
    "    label_to_synset = {label: wn.synset(synset) for label, synset in label_synsets if synset is not None}\n",
    "    abstraction_graph = {}\n",
    "    for label, synset in label_to_synset.items():\n",
    "        if synset in abstraction_graph: \n",
    "            continue\n",
    "        children = get_related_words(synset, task['down_fn'], root=task['root'], include_self=True)\n",
    "        children.add(label)\n",
    "        parents = get_related_words(synset, task['up_fn'], root=task['root'], include_self=False)\n",
    "        abstraction_graph[label] = {\n",
    "            'children': children,\n",
    "            'parents': parents\n",
    "        }\n",
    "    return abstraction_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b350ab5-9d50-4b04-a15f-1904bf3962fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related words for 105 synsets.\n",
      "Avg num children = 18.60952380952381\n",
      "Avg num parents = 3.5904761904761906\n",
      "announcer --> {'children': {'radio_announcer', 'sports_announcer', 'announcer', 'newscaster', 'newsreader', 'tv_announcer'}, 'parents': {'person', 'broadcaster', 'communicator'}}\n"
     ]
    }
   ],
   "source": [
    "abstraction_graph = get_abstraction_graph(task)\n",
    "print(f\"Related words for {len(abstraction_graph)} synsets.\")\n",
    "print(f\"Avg num children = {np.mean([len(w['children']) for w in abstraction_graph.values()])}\")\n",
    "print(f\"Avg num parents = {np.mean([len(w['parents']) for w in abstraction_graph.values()])}\")\n",
    "print(list(abstraction_graph.keys())[0], '-->', abstraction_graph[list(abstraction_graph.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c263b9b-e37a-46a9-b031-32ca1069f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1601 words related to the task.\n"
     ]
    }
   ],
   "source": [
    "task_words = {'children': set([]), 'parents': set([])}\n",
    "for words in abstraction_graph.values():\n",
    "    task_words['children'].update(words['children'])\n",
    "    task_words['parents'].update(words['parents'])\n",
    "print(f\"{len(task_words['children']) + len(task_words['parents'])} words related to the task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89af21-5565-4efa-9951-4ede56df6fe5",
   "metadata": {},
   "source": [
    "### Compute abstraction alignment specificity metrics\n",
    "We test two additional specificity metrics, p_s = P(s_s&#8595;, s_&#8593;) and p_t = P(s_s&#8597;, s_t)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42958adf-4941-45a9-b459-6d790fcf14d4",
   "metadata": {},
   "source": [
    "p_s = P(s_s&#8595;, s_&#8593;) measures how often a specific word is preffered to a general word. To compute this we compare all words more specific than the specific label to all words more general than the specific label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b67022a-e9cc-4d79-8678-dda97e503544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_s --> how often a specific word is preferred to a general word\n",
    "def compute_ps(results, data, abstraction_graph, agg_fn=np.max):\n",
    "    \"\"\"Computes how often a specific answer is preferred to a general answer.\"\"\"\n",
    "    prefers_specific = 0\n",
    "    num_instances = 0\n",
    "    for i, result in enumerate(results):\n",
    "        subject = result['sample']['sub_label']\n",
    "        label = data[subject]['obj_label']\n",
    "        if label not in abstraction_graph:\n",
    "            continue\n",
    "        specific_word_probs = []\n",
    "        general_word_probs = []\n",
    "        for token in result['masked_topk']['topk']:\n",
    "            if token['token_word_form'] in abstraction_graph[label]['children']:\n",
    "                specific_word_probs.append(np.exp(token['log_prob']))\n",
    "            if token['token_word_form'] in abstraction_graph[label]['parents']:\n",
    "                general_word_probs.append(np.exp(token['log_prob']))     \n",
    "            \n",
    "        if len(specific_word_probs) == 0 or len(general_word_probs) == 0:\n",
    "            continue # no related words are in the vocab, so can't copute ps\n",
    "        num_instances += 1\n",
    "        \n",
    "        if agg_fn(specific_word_probs) >= agg_fn(general_word_probs):\n",
    "            prefers_specific += 1\n",
    "    return prefers_specific / num_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "318bf877-1cba-4f8b-a3c7-9586072e7b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base occupation prediction ps_max = 79.01%\n"
     ]
    }
   ],
   "source": [
    "ps_max = compute_ps(results, data, abstraction_graph, agg_fn=np.max)\n",
    "print(f\"{model} {task['name']} prediction ps_max = {ps_max:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26427ac-4fb9-4b76-934a-01059a27ca25",
   "metadata": {},
   "source": [
    "p_t = P(s_s&#8597;, s_t) measures how often a related word is preffered to a topic word. We compute it by comparing all words related to the label to all other wrods related to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aac5f3ae-9753-4ed6-855c-670b01c0ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pt(results, data, abstraction_graph, task_words, agg_fn=np.max):\n",
    "    \"\"\"Computes how often a related word is prefferred to a topic word.\"\"\"\n",
    "    prefers_specific = 0\n",
    "    num_instances = 0\n",
    "    for result in results:\n",
    "        subject = result['sample']['sub_label']\n",
    "        label = data[subject]['obj_label']\n",
    "        if label not in abstraction_graph:\n",
    "            continue\n",
    "        specific_word_probs = []\n",
    "        task_word_probs = []\n",
    "        for token in result['masked_topk']['topk']:\n",
    "            token_word = token['token_word_form']\n",
    "            if token_word in abstraction_graph[label]['children'] or token_word in abstraction_graph[label]['parents']:\n",
    "                specific_word_probs.append(np.exp(token['log_prob']))\n",
    "            if token_word in task_words:\n",
    "                task_word_probs.append(np.exp(token['log_prob']))\n",
    "            \n",
    "        if len(specific_word_probs) == 0 or len(task_word_probs) == 0:\n",
    "            continue # no related words are in the vocab, so can't copute ps\n",
    "        num_instances += 1\n",
    "        \n",
    "        if agg_fn(specific_word_probs) >= agg_fn(task_word_probs):\n",
    "            prefers_specific += 1\n",
    "            \n",
    "    # print(f'Computing over {num_instances}/{len(data)} instances')\n",
    "    return prefers_specific / num_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9d7695b-9078-4fed-a790-2d7a6879c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_words = set([])\n",
    "for words in abstraction_graph.values():\n",
    "    task_words.update(words['children'])\n",
    "    task_words.update(words['parents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fddd860b-e460-4a89-8204-e2bc8c677662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base occupation prediction pt_max = 0.68%\n"
     ]
    }
   ],
   "source": [
    "pt_max = compute_pt(results, data, abstraction_graph, task_words, agg_fn=np.max)\n",
    "print(f\"{model} {task['name']} prediction pt_max = {pt_max:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6469e30-4109-41c3-85a4-89bf24863dc7",
   "metadata": {},
   "source": [
    "### Compute all specificity metrics for all pairs of tasks and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e27583-e32a-41c6-a460-8944220e3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(tasks, models):\n",
    "    for task in tasks:\n",
    "        print(f\"TASK {task['id']}: {task['name']} prediction\")\n",
    "        data = load_data(task['id'])\n",
    "        abstraction_graph = get_abstraction_graph(task)        \n",
    "        task_words = set([])\n",
    "        for words in abstraction_graph.values():\n",
    "            task_words.update(words['children'])\n",
    "            task_words.update(words['parents'])\n",
    "        for model in models:\n",
    "            print(f\"MODEL {model}\")\n",
    "            results = load_model_results(model, task['id'])\n",
    "\n",
    "            accuracy_10 = compute_accuracy(results, data, k=10)\n",
    "            print(f'--- acc@10 = {accuracy_10:.2%}')\n",
    "            \n",
    "            pr = compute_pr(results, data)\n",
    "            print(f'--- pr     = {pr:.2%}')\n",
    "            \n",
    "            ps_max = compute_ps(results, data, abstraction_graph, agg_fn=np.max)\n",
    "            print(f\"--- ps_max = {ps_max:.2%}\")\n",
    "            \n",
    "            pt_max = compute_pt(results, data, abstraction_graph, task_words, agg_fn=np.max)\n",
    "            print(f\"--- pt_max = {pt_max:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cad9e5bf-0d6e-4bb8-86fc-b79cc778b5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK P106: occupation prediction\n",
      "MODEL bert_base\n",
      "--- acc@10 = 28.44%\n",
      "--- pr     = 70.46%\n",
      "--- ps_max = 79.01%\n",
      "Computing over 4994/4999 instances\n",
      "--- pt_max = 0.68%\n",
      "MODEL bert_large\n",
      "--- acc@10 = 22.14%\n",
      "--- pr     = 71.76%\n",
      "--- ps_max = 82.40%\n",
      "Computing over 4994/4999 instances\n",
      "--- pt_max = 1.16%\n",
      "MODEL roberta.base\n",
      "--- acc@10 = 24.50%\n",
      "--- pr     = 61.80%\n",
      "--- ps_max = 78.97%\n",
      "Computing over 4994/4999 instances\n",
      "--- pt_max = 7.51%\n",
      "MODEL roberta.large\n",
      "--- acc@10 = 22.44%\n",
      "--- pr     = 71.44%\n",
      "--- ps_max = 82.38%\n",
      "Computing over 4994/4999 instances\n",
      "--- pt_max = 7.97%\n",
      "MODEL gpt2\n",
      "--- acc@10 = 16.10%\n",
      "--- pr     = 57.28%\n",
      "--- ps_max = 51.92%\n",
      "Computing over 4994/4999 instances\n",
      "--- pt_max = 16.84%\n",
      "TASK P131: location prediction\n",
      "MODEL bert_base\n",
      "--- acc@10 = 43.16%\n",
      "--- pr     = 49.09%\n",
      "--- ps_max = 95.91%\n",
      "Computing over 4519/4976 instances\n",
      "--- pt_max = 23.04%\n",
      "MODEL bert_large\n",
      "--- acc@10 = 45.64%\n",
      "--- pr     = 42.36%\n",
      "--- ps_max = 97.04%\n",
      "Computing over 4519/4976 instances\n",
      "--- pt_max = 27.44%\n",
      "MODEL roberta.base\n",
      "--- acc@10 = 36.59%\n",
      "--- pr     = 49.99%\n",
      "--- ps_max = 97.59%\n",
      "Computing over 4519/4976 instances\n",
      "--- pt_max = 17.90%\n",
      "MODEL roberta.large\n",
      "--- acc@10 = 39.05%\n",
      "--- pr     = 43.28%\n",
      "--- ps_max = 97.85%\n",
      "Computing over 4519/4976 instances\n",
      "--- pt_max = 22.42%\n",
      "MODEL gpt2\n",
      "--- acc@10 = 17.02%\n",
      "--- pr     = 48.25%\n",
      "--- ps_max = 44.89%\n",
      "Computing over 4519/4976 instances\n",
      "--- pt_max = 13.48%\n",
      "TASK P19: place of birth prediction\n",
      "MODEL bert_base\n",
      "--- acc@10 = 41.42%\n",
      "--- pr     = 60.68%\n",
      "--- ps_max = 99.92%\n",
      "Computing over 4711/4994 instances\n",
      "--- pt_max = 24.50%\n",
      "MODEL bert_large\n",
      "--- acc@10 = 42.14%\n",
      "--- pr     = 56.52%\n",
      "--- ps_max = 99.67%\n",
      "Computing over 4711/4994 instances\n",
      "--- pt_max = 25.92%\n",
      "MODEL roberta.base\n",
      "--- acc@10 = 28.97%\n",
      "--- pr     = 54.48%\n",
      "--- ps_max = 100.00%\n",
      "Computing over 4711/4994 instances\n",
      "--- pt_max = 20.42%\n",
      "MODEL roberta.large\n",
      "--- acc@10 = 23.21%\n",
      "--- pr     = 42.16%\n",
      "--- ps_max = 99.89%\n",
      "Computing over 4711/4994 instances\n",
      "--- pt_max = 22.48%\n",
      "MODEL gpt2\n",
      "--- acc@10 = 33.27%\n",
      "--- pr     = 59.72%\n",
      "--- ps_max = 98.45%\n",
      "Computing over 4711/4994 instances\n",
      "--- pt_max = 19.59%\n"
     ]
    }
   ],
   "source": [
    "compute_all_metrics(TASKS, MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732c258-2195-4565-b31c-3ed3c6f6ae57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
