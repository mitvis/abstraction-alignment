{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d186ff-ec39-4566-ab49-4be34aecfc2a",
   "metadata": {},
   "source": [
    "# Abstraction Alignment to Interpret a CIFAR-100 Model\n",
    "We apply abstraction alignment to interpret a CIFAR-100 image classification model using the CIFAR-100 class-superclass hierarchy as the human abstraction graph. This example is loosly based on the Interpreting Image Model Behavior case study in the Abstraction Alignment paper (Section 5.1).\n",
    "\n",
    "In this notebook, we train and evaluate a ResNet20 on CIFAR-100. We use the abstraction alignment methodology to aggregate and report the model's abstraction match and qualitatively analyze common \"types\" of abstraction (mis)alignment.\n",
    "\n",
    "Note: the values in this notebook may differ slightly from the values in the interface due to differences in normalization and thresholding. The relative values and takeaways remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c600f05-c1a4-4bb7-b493-31b9f5dc96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80be5757-c011-490a-9928-8f91b451f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "from itertools import combinations\n",
    "\n",
    "from abstraction_graph_cifar import make_abstraction_graph, show_abstraction_graph, propagate\n",
    "import util.cifar.cifar_util as cifar_util\n",
    "import util.cifar.cifar_train as cifar_train\n",
    "import util.cifar.cifar_metadata as cifar_metadata\n",
    "\n",
    "import metrics\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe1308-c6a1-4f0d-b7dc-cc023a987e00",
   "metadata": {},
   "source": [
    "## CIFAR-100 Model and Dataset\n",
    "First, we load the CIFAR-100 dataset and train a ResNet20 on it. This is the model we will inspect with abstraction alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dbf4bc3-03d6-4ce0-85fd-181c7ad111ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined paths -- TODO: update with your own\n",
    "MODEL_PATH = 'util/cifar/'\n",
    "CIFAR_DIR = '/nobackup/users/aboggust/data/cifar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb68d8ff-7f5c-4ec3-ac8a-e2269d00f094",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained CIFAR-100 model from: util/cifar/resnet20/checkpoints/checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "# Train a ResNet20 on CIFAR-100\n",
    "batch_size = 128\n",
    "data_augmentation = True\n",
    "epochs = 200\n",
    "architecture = 'resnet20'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = os.path.join(MODEL_PATH, architecture, 'checkpoints', 'checkpoint.pt')\n",
    "if os.path.isfile(checkpoint):\n",
    "    print(f'Loading trained CIFAR-100 model from: {checkpoint}')\n",
    "    model = cifar_util.load_model(architecture)\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "else:\n",
    "    print(f'Training CIFAR-100 model')\n",
    "    model = cifar_train.train(\n",
    "        architecture, \n",
    "        batch_size, \n",
    "        epochs, \n",
    "        CIFAR_DIR,\n",
    "        MODEL_PATH,\n",
    "        data_augmentation\n",
    "    )\n",
    "    \n",
    "model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e0388f-bf30-4361-8316-7d772b42b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-100 dataset for exploration\n",
    "train_loader, test_loader = cifar_util.load_dataset(CIFAR_DIR, \n",
    "                                                    data_augmentation,\n",
    "                                                    batch_size)\n",
    "test_dataset = test_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86afa4b2-33d7-4f75-b2cc-f0e45794e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:05<00:00, 14.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute model outputs on test instances\n",
    "labels = []\n",
    "outputs = []\n",
    "for i, (image, label) in enumerate(tqdm(test_loader)):\n",
    "    with torch.no_grad():\n",
    "        images = image.to(device)\n",
    "    labels.extend(label.numpy())\n",
    "    output = model(images)\n",
    "    output = torch.nn.functional.softmax(output, dim=1).detach().cpu().numpy()\n",
    "    outputs.append(output)\n",
    "outputs = np.vstack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b313015-bd94-40a2-be71-bb35cdde4358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL STATS:\n",
      "Accuracy: 67.68%\n",
      "Mean prediction confidence: 0.78\n",
      "Mean prediction confidence (correct): 0.87\n",
      "Mean prediction confidence (incorrect): 0.59\n"
     ]
    }
   ],
   "source": [
    "# Print model performance statistics\n",
    "predictions = [np.argmax(output) for output in outputs]\n",
    "correctness = [label == predictions[i] for i, label in enumerate(labels)]\n",
    "correct_inds = [i for i, label in enumerate(labels) if label == predictions[i]]\n",
    "incorrect_inds = [i for i, label in enumerate(labels) if label != predictions[i]]\n",
    "print(f'MODEL STATS:')\n",
    "print(f'Accuracy: {len(correct_inds) / len(labels):.2%}')\n",
    "print(f'Mean prediction confidence: {np.mean([np.max(output) for output in outputs]):.2f}')\n",
    "print(f'Mean prediction confidence (correct): {np.mean([np.max(outputs[i]) for i in correct_inds]):.2f}')\n",
    "print(f'Mean prediction confidence (incorrect): {np.mean([np.max(outputs[i]) for i in incorrect_inds]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279f9f2-a94d-4a79-9650-d329545c22d2",
   "metadata": {},
   "source": [
    "## Compute Abstraction Alignment\n",
    "We use abstraction alignment to analyze how well the human abstractions account for the model's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10298861-174d-4e19-ab72-4337314aa2e7",
   "metadata": {},
   "source": [
    "### Load the human abstraction graph (i.e., the CIFAR-100 class and superclass hiearchy)\n",
    "The human abstraction graph represents human concepts and the relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9a76c9-b727-49c1-a22b-1c7d6bc00eb6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 abstraction_graph with 121 nodes across 3 levels.\n",
      "root\n",
      "├── aquatic_mammals\n",
      "│   ├── beaver\n",
      "│   ├── dolphin\n",
      "│   ├── otter\n",
      "│   ├── seal\n",
      "│   └── whale\n",
      "├── fish\n",
      "│   ├── aquarium_fish\n",
      "│   ├── flatfish\n",
      "│   ├── ray\n",
      "│   ├── shark\n",
      "│   └── trout\n",
      "├── flowers\n",
      "│   ├── orchid\n",
      "│   ├── poppy\n",
      "│   ├── rose\n",
      "│   ├── sunflower\n",
      "│   └── tulip\n",
      "├── food_containers\n",
      "│   ├── bottle\n",
      "│   ├── bowl\n",
      "│   ├── can\n",
      "│   ├── cup\n",
      "│   └── plate\n",
      "├── fruit_and_vegetables\n",
      "│   ├── apple\n",
      "│   ├── mushroom\n",
      "│   ├── orange\n",
      "│   ├── pear\n",
      "│   └── sweet_pepper\n",
      "├── household_electrical_devices\n",
      "│   ├── clock\n",
      "│   ├── keyboard\n",
      "│   ├── lamp\n",
      "│   ├── telephone\n",
      "│   └── television\n",
      "├── household_furniture\n",
      "│   ├── bed\n",
      "│   ├── chair\n",
      "│   ├── couch\n",
      "│   ├── table\n",
      "│   └── wardrobe\n",
      "├── insects\n",
      "│   ├── bee\n",
      "│   ├── beetle\n",
      "│   ├── butterfly\n",
      "│   ├── caterpillar\n",
      "│   └── cockroach\n",
      "├── large_carnivores\n",
      "│   ├── bear\n",
      "│   ├── leopard\n",
      "│   ├── lion\n",
      "│   ├── tiger\n",
      "│   └── wolf\n",
      "├── large_man-made_outdoor_things\n",
      "│   ├── bridge\n",
      "│   ├── castle\n",
      "│   ├── house\n",
      "│   ├── road\n",
      "│   └── skyscraper\n",
      "├── large_natural_outdoor_scenes\n",
      "│   ├── cloud\n",
      "│   ├── forest\n",
      "│   ├── mountain\n",
      "│   ├── plain\n",
      "│   └── sea\n",
      "├── large_omnivores_and_herbivores\n",
      "│   ├── camel\n",
      "│   ├── cattle\n",
      "│   ├── chimpanzee\n",
      "│   ├── elephant\n",
      "│   └── kangaroo\n",
      "├── medium_mammals\n",
      "│   ├── fox\n",
      "│   ├── porcupine\n",
      "│   ├── possum\n",
      "│   ├── raccoon\n",
      "│   └── skunk\n",
      "├── non-insect_invertebrates\n",
      "│   ├── crab\n",
      "│   ├── lobster\n",
      "│   ├── snail\n",
      "│   ├── spider\n",
      "│   └── worm\n",
      "├── people\n",
      "│   ├── baby\n",
      "│   ├── boy\n",
      "│   ├── girl\n",
      "│   ├── man\n",
      "│   └── woman\n",
      "├── reptiles\n",
      "│   ├── crocodile\n",
      "│   ├── dinosaur\n",
      "│   ├── lizard\n",
      "│   ├── snake\n",
      "│   └── turtle\n",
      "├── small_mammals\n",
      "│   ├── hamster\n",
      "│   ├── mouse\n",
      "│   ├── rabbit\n",
      "│   ├── shrew\n",
      "│   └── squirrel\n",
      "├── trees\n",
      "│   ├── maple_tree\n",
      "│   ├── oak_tree\n",
      "│   ├── palm_tree\n",
      "│   ├── pine_tree\n",
      "│   └── willow_tree\n",
      "├── vehicles_1\n",
      "│   ├── bicycle\n",
      "│   ├── bus\n",
      "│   ├── motorcycle\n",
      "│   ├── pickup_truck\n",
      "│   └── train\n",
      "└── vehicles_2\n",
      "    ├── lawn_mower\n",
      "    ├── rocket\n",
      "    ├── streetcar\n",
      "    ├── tank\n",
      "    └── tractor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstraction_graph = make_abstraction_graph()\n",
    "print(f'CIFAR-100 abstraction_graph with {abstraction_graph.size()} nodes across {abstraction_graph.depth() + 1} levels.')\n",
    "print(show_abstraction_graph(abstraction_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbb14d-1115-4cd1-84bc-eb13163d7f50",
   "metadata": {},
   "source": [
    "### Create the model's fitted abstractions\n",
    "The model's fitted abstractions are a weighted version of the abstraction graph for each model decision. The value of each node represents the model's confidence in that concept. If it is a leaf node, then the value is the model's confidence in that decision. If it is an internal node, then the value is the sum of the model's confidence across its reacahble leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e03894-e9a2-4367-834b-96e457a30723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:10<00:00, 76.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the model's fitted abstractions for each instance by propagating model outputs through the abstraction graph\n",
    "fitted_abstractions = []\n",
    "for i in tqdm(range(len(labels))):\n",
    "    fitted_abstraction = propagate(outputs[i], make_abstraction_graph())\n",
    "    fitted_abstractions.append(fitted_abstraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5319d3d7-c975-4dbb-9c3c-aed2e4f8dfc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP PREDICTED CLASSES:  ['train (0.37)', 'road (0.18)', 'spider (0.17)']\n",
      "FITTED ABSTRACTION:\n",
      "root (1.00)\n",
      "├── aquatic_mammals (0.03)\n",
      "│   ├── seal (0.02)\n",
      "├── household_furniture (0.01)\n",
      "│   ├── couch (0.01)\n",
      "├── insects (0.03)\n",
      "│   └── cockroach (0.03)\n",
      "├── large_man-made_outdoor_things (0.20)\n",
      "│   ├── bridge (0.01)\n",
      "│   ├── road (0.18)\n",
      "│   └── skyscraper (0.01)\n",
      "├── large_natural_outdoor_scenes (0.01)\n",
      "├── large_omnivores_and_herbivores (0.01)\n",
      "├── non-insect_invertebrates (0.17)\n",
      "│   ├── spider (0.17)\n",
      "├── people (0.01)\n",
      "│   ├── girl (0.01)\n",
      "├── trees (0.12)\n",
      "│   ├── palm_tree (0.12)\n",
      "├── vehicles_1 (0.37)\n",
      "│   └── train (0.37)\n",
      "└── vehicles_2 (0.04)\n",
      "    ├── streetcar (0.04)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show an example fitted abstraction\n",
    "index = 0\n",
    "\n",
    "image = test_dataset[index][0].permute(1, 2, 0)\n",
    "plt.imshow(cifar_util.unnorm_cifar_image(image))\n",
    "plt.axis('off')\n",
    "plt.title(f'Label: {cifar_metadata.CLASS_LABELS[labels[index]]}\\nPred: {cifar_metadata.CLASS_LABELS[predictions[index]]}')\n",
    "plt.show()\n",
    "\n",
    "top_predictions = np.argsort(outputs[index])[-3:]\n",
    "print('TOP PREDICTED CLASSES: ', [f'{cifar_metadata.CLASS_LABELS[i]} ({outputs[index][i]:.2f})' for i in top_predictions][::-1])\n",
    "\n",
    "print('FITTED ABSTRACTION:')\n",
    "print(show_abstraction_graph(fitted_abstractions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838c642-092d-4484-980c-efa473037f5c",
   "metadata": {},
   "source": [
    "### Abstraction Match\n",
    "Abstraction match measures how much moving up one level of abstraction reduces the model's entropy. If the abstraction match is high (e.g., a lot of entropy is reduced), that is a signal that the model has learned those abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eae3ec9-35d9-42ff-bab4-59fa57eeeb89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:37<00:00,  1.89s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-53d44c93fcc443c79eda9fcef798224d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-53d44c93fcc443c79eda9fcef798224d.vega-embed details,\n",
       "  #altair-viz-53d44c93fcc443c79eda9fcef798224d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-53d44c93fcc443c79eda9fcef798224d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-53d44c93fcc443c79eda9fcef798224d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-53d44c93fcc443c79eda9fcef798224d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.8.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.8.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-6396fcf7d066a9c0e7cfeccd6da02f5b\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"concept\", \"type\": \"nominal\"}, \"x\": {\"field\": \"concept\", \"sort\": \"-y\", \"type\": \"nominal\"}, \"y\": {\"field\": \"abstraction match\", \"type\": \"quantitative\"}}, \"title\": \"Abstraction Match per Superclass\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.8.0.json\", \"datasets\": {\"data-6396fcf7d066a9c0e7cfeccd6da02f5b\": [{\"concept\": \"people\", \"abstraction match\": 0.6951291561126709}, {\"concept\": \"trees\", \"abstraction match\": 0.666723906993866}, {\"concept\": \"flowers\", \"abstraction match\": 0.5305652618408203}, {\"concept\": \"household_furniture\", \"abstraction match\": 0.4494735300540924}, {\"concept\": \"fruit_and_vegetables\", \"abstraction match\": 0.4099380075931549}, {\"concept\": \"large_natural_outdoor_scenes\", \"abstraction match\": 0.40929871797561646}, {\"concept\": \"food_containers\", \"abstraction match\": 0.39550936222076416}, {\"concept\": \"aquatic_mammals\", \"abstraction match\": 0.38348305225372314}, {\"concept\": \"small_mammals\", \"abstraction match\": 0.36819955706596375}, {\"concept\": \"large_man-made_outdoor_things\", \"abstraction match\": 0.35700759291648865}, {\"concept\": \"fish\", \"abstraction match\": 0.3563827574253082}, {\"concept\": \"insects\", \"abstraction match\": 0.33431577682495117}, {\"concept\": \"large_omnivores_and_herbivores\", \"abstraction match\": 0.3260774612426758}, {\"concept\": \"household_electrical_devices\", \"abstraction match\": 0.3251038193702698}, {\"concept\": \"vehicles_1\", \"abstraction match\": 0.32439184188842773}, {\"concept\": \"large_carnivores\", \"abstraction match\": 0.3212800621986389}, {\"concept\": \"reptiles\", \"abstraction match\": 0.2922455966472626}, {\"concept\": \"medium_mammals\", \"abstraction match\": 0.29131749272346497}, {\"concept\": \"non-insect_invertebrates\", \"abstraction match\": 0.28360676765441895}, {\"concept\": \"vehicles_2\", \"abstraction match\": 0.2339886575937271}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute level-1 abstraction match aggregated per level-1 concept\n",
    "instance_per_concept = {}\n",
    "for i, label in enumerate(labels):\n",
    "    label_concept = cifar_metadata.CLASS_TO_SUPERCLASS[label]\n",
    "    if label_concept not in instance_per_concept:\n",
    "        instance_per_concept[label_concept] = set([])\n",
    "    instance_per_concept[label_concept].add(i)\n",
    "\n",
    "# Compute abstraction match per leve-1 concept\n",
    "level_1_concept_abstraction_match = {}\n",
    "for concept, instances in tqdm(instance_per_concept.items()):\n",
    "    instance_fitted_abstractions = [fitted_abstraction for i, fitted_abstraction in enumerate(fitted_abstractions) if i in instances]\n",
    "    match = metrics.abstraction_match(instance_fitted_abstractions, 1)\n",
    "    level_1_concept_abstraction_match[concept] = match\n",
    "        \n",
    "# Plot abstraction match\n",
    "ordered_concepts = sorted(level_1_concept_abstraction_match.keys(), key=lambda c: level_1_concept_abstraction_match[c], reverse=True)\n",
    "df = pd.DataFrame.from_dict({\n",
    "    'concept': [cifar_metadata.SUPERCLASS_LABELS[concept] for concept in ordered_concepts],\n",
    "    'abstraction match': [level_1_concept_abstraction_match[concept] for concept in ordered_concepts],\n",
    "})\n",
    "alt.Chart(df, title='Abstraction Match per Superclass').mark_bar().encode(\n",
    "    x=alt.X('concept:N').sort('-y'),\n",
    "    y=alt.Y('abstraction match:Q'),\n",
    "    color=alt.Color('concept:N'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a8cfd6-dfd7-43e0-8f71-d792859d6dbd",
   "metadata": {},
   "source": [
    "### Query for model behavior types\n",
    "We can use abstraction alignment to query for common pattens in model behavior. We define a query over the fitted abstractions and return all the instances that match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e40c3529-edac-401c-9b3a-e16adb91573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(indicies, num_examples):\n",
    "    if len(indicies) > 0:\n",
    "        example_inds = np.random.choice(indicies, num_examples)\n",
    "        plot_images(example_inds)\n",
    "        plt.show()\n",
    "        print(show_abstraction_graph(fitted_abstractions[example_inds[0]]))\n",
    "\n",
    "def plot_images(indicies):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(indicies))\n",
    "    if type(ax) != list:\n",
    "        ax = [ax]\n",
    "    for i, index in enumerate(indicies):\n",
    "        ax[i].imshow(cifar_util.unnorm_cifar_image(test_dataset[index][0].permute(1, 2, 0)))\n",
    "        ax[i].axis('off')\n",
    "    return fig\n",
    "\n",
    "def get_nodes_at_level(abstraction_graph, level, non_zero=True):\n",
    "    nodes = abstraction_graph.filter_nodes(lambda x: abstraction_graph.depth(x) == level)\n",
    "    if non_zero:\n",
    "        nodes = [node for node in nodes if node.data is not None and node.data >= 0.01]\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60c5cc4b-7910-4d2a-b3dd-03b0ce395213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:12<00:00, 816.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2976 images where the model is fully confident in one output concept.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root (1.00)\n",
      "├── medium_mammals (1.00)\n",
      "│   └── skunk (1.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fully confident in an output concept\n",
    "fully_confident = []\n",
    "for i, fitted_abstraction in enumerate(tqdm(fitted_abstractions)):\n",
    "    non_zero_leaf_nodes = get_nodes_at_level(fitted_abstraction, 2, True)\n",
    "    if len(non_zero_leaf_nodes) == 1:\n",
    "        fully_confident.append(i)\n",
    "print(f'{len(fully_confident)} images where the model is fully confident in one output concept.')\n",
    "show_examples(fully_confident, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "078b14ef-a4cd-4f3e-8c7c-bd32b984a4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:38<00:00, 261.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 images where the model is split between two branches through two superclass.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVP0lEQVR4nO2dWW9kaVKGI/fdzsV22undVa5yt6vcUy3U0LO0ekZCrblAoxkBfwAk/g43XHPBzVxMA5fAALNAi0XDUFVdVXa5usr77lyd+8ofiDekRkgdQu9z+YU+58lz8vWR4v0iIjCZTCZCCHFH8Ju+AEKIDsVJiFMoTkKcQnES4hSKkxCnhK3gze05jLXbHRg7PT3V/97NDdwzGo1gLJlMwth4PIaxSrWirl9fX8M9q6urMJZOp2Gs1W7B2MXFGYyFQvoj6Hb6cM+7gyMYS6USMDYa43tcnJ1T14NB/P87ly/C2OaDLRgTwdex/2ZPXb++uoJ7CrlZ/EmjIYz95r9+C2OpFP7NPXr8WF237tVwiK/jz/7kT/W/B3cQQr5RKE5CnEJxEuIUipMQp1CchDiF4iTEKaaV0my1YazX7cJYvV5X1zsdbL9Eo1EYGwwGMJZKpWAsFo2p64XCDNxTq+nXLiLSbGK7JJ/Pw1ihUMCfV9U/r9fDVko8Hoexu9YdjNVrVRjrdPVnHQzh/9997A5INoe/czQagrFCQb+PhXwO7rk8xzbL2LCP3n/vIYwNBvjLBQIBdT0Uwt/LsgoRfHMS4hSKkxCnUJyEOIXiJMQpFCchTqE4CXGKaaW0mk0Y6xpWSjis/1nLAnj9+vXX/nsiIouLizBWrzfU9XQmA/dYVkq5rFe5iIjEYrptIyIyEdymaa6oV4NUKvg67u6wXdIZYPtrZFxHCzzPkGGlIEtBRKRSKcNYNIqf59S0bo1FIhG4x6rEOT09gTG7fRb+3q2WbqlZdqD1zL7+FRBCvlEoTkKcQnES4hSKkxCnUJyEOMXM1jYaerZTRKTfxwezURavaWR/X716BWPb29swljYOvpcr+kHvZhNnNC8ucX+hw4MDGGsb2ev5Ij4Un8vpB7qtTOjQOEQdT+BMdHFhGca6Xb0oIRLBGchYAvfZ6XTw/ej1cJZ0MOyp66kkzvR3Ovh5Wpl+K4sei+LPmyvqvZNQwYeIfSgewTcnIU6hOAlxCsVJiFMoTkKcQnES4hSKkxCn2AffwQFfEbv1PEpfB4108s7ODow9ePAAxqamp2FMjvWxELt7+3DLORglISJydo7HU1j9haKR92AsEceHthFWv6JoEo+M6PVwL6b1tU11fTTC4y6GQxyTIO7BMxxiGw5ZKZ02tuHuGtjCmJ3FoxqsYoV6HR9UR6Ma/q/nUPPNSYhTKE5CnEJxEuIUipMQp1CchDiF4iTEKaaV0jYqC1Ix3NOlL3pFxdz8PNyTSeFqiuYdTqPXa7hyBvYKCmALwOpztLKyAWM3FVzNsvME20STkX6v2qBKREQkEsL3fnMVX+PuPq78SSV0C+bsEk8jL1cuYWyltAZjLaMqaNTXv/e5YWNNZ3FlUrGIf3PhEK646XZ1S0dE5NWrL9X1bBaPjEgm8DUi+OYkxCkUJyFOoTgJcQrFSYhTKE5CnEJxEuIU00qpGpOQ//PLZzCWzulVEx9+/G245+ICp+WtZmKDPq60SGT06oF8Pgv3bCyvw9jVLa5+eHdyDGPXZVyxUi/rNlHPsFLqVZzmvzjEn9VpYGusdadbUqcnR3DP5S2OFYt4TEbXeJ7jof69p43qo2QCV/YYg63N5mXr62swdnSkN3oLGg3DrEZ0CL45CXEKxUmIUyhOQpxCcRLiFIqTEKdQnIQ4xZ6VYlgpey9ewNj8yqq6vnYfN+oaGzlvK3ZxdQFjly+u9OvYwJUb9Sq2S379z7+AsZNjXL3xF3+O902n9bkb3TaeDJ3N4EqLm1lc8dEc1mAsnrlV1+tVXPWzuYmf5/U1tsbSaVz50x/rNkvUmMsSCuJGXYMBbjRWLuNKonIFP89GQ7erkqvYLrm+wZ+F4JuTEKdQnIQ4heIkxCkUJyFOoTgJcYqZrS3f6hk8EZFNI+M5v6Afeg4K7t0TieJLub7Rs64iInfgwLaIyNLSkroei+Ds3t/89d/C2Lt3JzA2DOFM3TCCD6pHwJnt6yrumzSY4IxsegpntmttvO/vfv4rdT0RnYF7Ykbfp1AQHyo/OMDPc35Jz8oW8/j+9lq4+MEaJ3F8cghjBwd4ZEckql9jMmkcbp98/fcg35yEOIXiJMQpFCchTqE4CXEKxUmIUyhOQpxiWikPN+/B2Ff1GozdXuqH0Tceb8M96QX9ALiISLWKD4G///5DGJsrLqjrP/3p53DP/lu9P4yIyDiMLRhJ6WMVRESaEdxfqFnTD4gPQ9hK6XawxRWo4X3NNraCbi8r6no2gw+pX9RewtjKyn0YK1/jgortx/oU8LlCFu55c7MLYweHhzDW6eIiB6u/0PSU3s/IGhmRyxZgDME3JyFOoTgJcQrFSYhTKE5CnEJxEuIUipMQp5hWSj6bhbGv9l/DWL16p67vfOdjuGd75zGMZbO4FX8shlPez57qqf5nT/EoiWEQWyLjBP6sZAlXb4yjxriAfkhdjwr+rOAYWzrnXTwiod+rwVi8oKf6hwE8wqEvePTDu0tceRIe4MqZRlW3WWKCr+PqAltE8QS+V/MLuBfTytIKjEUi+mTxc2OkyMiaCwHgm5MQp1CchDiF4iTEKRQnIU6hOAlxCsVJiFNMK2V3D1cd5GdnYeyTT3+grq8sL8M9N7e48iQSxpdptdvffa3bPTWjciMY1tPkIiLjOI5JGE81liC2AUR3UkQC+HuNB3gydH+Am4n1W8Z1BGrqcjiOLYBuC1d1JOemYCwk+Lu93X+urmceb8E9mek0jJ0aE8dDxu9qbhZXSZ2cnYEItuH6xjND8M1JiFMoTkKcQnES4hSKkxCnUJyEOIXiJMQpppVyW9GbPomI7Hz8ezC2uKA31opEcYVAt92BsZMTlLoWiUZx9cZgCOZkGCn08QR5GyLBHk6Ht6/xJOSxkWIfg0nO1n/NcAhXuQSxSyHRHrZZgiP9OoZt/AfDATyHZHiDLYwxvsXSaZXU9WgUzyFJJLBF1GzpFVIiIpUybpQ2HBozZ+q6FWfNSplwVgoh/3+gOAlxCsVJiFMoTkKcQnES4hSKkxCnmFbKoIdT5W8OcCOpl6/fqOs7H3wL7pkymok1m7iRVK+Hq1kSoLnTyuoi3LP/Fts24z6uPBn28ejz8cj6H2h4H2hHCFs6UaM6Jp/BVta4p1sH4RielTI9jRuvVQ0bbmhYUqOhfh8HQ3yfAgFsVW29p89eERFJJXE1SyCAn1kyqY+dD4WwnEZfv78X35yEeIXiJMQpFCchTqE4CXEKxUmIU+xxDNN4Gu/zFy9g7L+fPVXXL29wBu/JkycwFo/hLGMgiP+/RGP613tvC0/srjXaRgwfzu+DLKOISDyKT3qHI+ARTIyeRMZjiwZxhn1tSS9IEBFpg4x4wBhPMVPEfXZiMdxvKZ/HWd7pbEZdTxqjMHLT+mF5EZFcHk8+Dwbxc7k4x4UMy0tL6vrpmT7RXUSk1cK/HQTfnIQ4heIkxCkUJyFOoTgJcQrFSYhTKE5CnGJaKbnpLIw9evwIxuIZ/UBxs4nHIBwd4enES4v4oLqVDm/e6Z8XCGCbYiaXhbFhH9sU+fwcjK2v4qnXQdD0Z2xYKWcneGp00jiovraOx2HclvUCgnqtBvfEDbskHjemgCfxNYro97jbxX2Cpo1xDAnDgplMsE00v4Btoj6YzB00bL2w0bcKwTcnIU6hOAlxCsVJiFMoTkKcQnES4hSKkxCn2PndCe7bsnFvFcZOr/U+PMDZEBGRfh9/1gCkrkVE4sa06U5bHz9weoT7BHVbuCqlNIerdJaXcep9cTEHY+Gwbh2k03p1hohIx7iRSwtrMFZYmIexaEYfJRA4xVZV5RaPM0iCvydiT98OBMHzNEY/jEa4IqjZbMBYvoCnsydB/ykRkREYrfD2He6rlclgSwfBNychTqE4CXEKxUmIUyhOQpxCcRLiFIqTEKeYVsrh0VsY++PvfgRj9U5dXd/dfQf35KdwWttq+1+v41R5u6VXMgRRul5EForYbsga1Q/pDK60iMfwvlJJ/95DY/zA48cfwtj7D7ZhrNzC9yqR1+2eVHoK7tl/8QrGAiH8f//e/XUYy2b1zyvk83DPgtFo7K5ZhbFKGTecm53FzdDSKd0mGgzwmInZAq5aQvDNSYhTKE5CnEJxEuIUipMQp1CchDiF4iTEKaaVcnaMm27dHJ/D2Oa6Pouk1cTVCMloAsaGI2wroKoOEZHSom6LWFUu1oiSqNHQypqu3O3hPzoRvVqh28MNrZ58+C0Ymy8azdDOcDVO5E63WeJiNMH63ndg7PAYV2isGA3bEkn9dzBTwE3SImFcQTLs4+s/ODzF+wZ4XyKp36tMCttpbWAvWvDNSYhTKE5CnEJxEuIUipMQp1CchDjFzNZG4zj79MWv/wXGvvfD31fXl0r4MHEogP9PXF7h8QOZTBLG0hn9EHW9jvsEdTp4AnEigTPK/T4+9Nxo3MHY3us36no2h3sIjcY429zq6BOqRUSiEZxtLl/pk5zDYWsqN451u/g+HhwcwNjq6or+WUH8U223cWa7bBxuv7nWR1CIiISMz1ss6ZO0s9P4me2/0Z+zBd+chDiF4iTEKRQnIU6hOAlxCsVJiFMoTkKcYlopD7e2YOyLX/wKxla3NtX1tQf6uojIaIQPh9cbuPfNyDgUX6noFoZ1iLrdwTbLzY1uN4iIxA3byXCJ5PLiQl3v93GRQCaFU/bjET6wbR3qT6V0SyqTwZ91W8bjGCq32KboJPA9zmb1XkbBALZthkNc/FCtWT2E8DVGDNupBMZaZLNZuCeXwyM5EHxzEuIUipMQp1CchDiF4iTEKRQnIU6hOAlximmlzC/iKpK7Nq5+OHynVx1899NP4Z6uUdVxB/rbiIicn+O+OMiCSadxa/+QMUagUsEVDr0etj5ixvTtblevqKiUa3BPPoerXDIZPPV6agrbIgsL+rO2qm0sa2Z94z6MWbZCtarf41QSj7TodvG9R/dXRGQwwBOxz05x/6zVlSV13bKdkklcPYXgm5MQp1CchDiF4iTEKRQnIU6hOAlxCsVJiFNMK2VpRW+2JCLykz/6Qxj711/qFSuNCq4QKC7jFv0Z0KhLRKTT+QrGwiH9600MD6DZwhZROIxvV71Wg7Gjo0sYG4GKilQSp+WbTauhFb7HhQK2kKpVfV+zia0ZMUY1PHzwEO8yynQq4Dfy1Vs8FX0BVImIiJQW9GZcIra90ajXYKwC7pX1+5gybBYE35yEOIXiJMQpFCchTqE4CXEKxUmIUyhOQpxiWiktY25IYdaaNKxXYfzTz/8R7vnhj38EY8bQaLHS+W1w/d0+tlJahpXSvMPVIDe31zDWaNRgbHFRt6umswW4JxbDM1uCRlXN8fExjLVaetOtldVluGcyxvfemjXSM6Z2JxN69Yk1b6bdxg3D8kYFzO9+9BGMPf/yOYydgKndqWQK7kkkcAM4BN+chDiF4iTEKRQnIU6hOAlxCsVJiFPMbO3QGJFwbRywninph9j//d++gHu2d7ZhrD/Grfjzedzn6Pyypq5XqniMgHV4OW5Mtu4ZfWwScdz/JhjUv1s8HoN70mmcFUyn8QHrUAhnV4NB/f/0xJiiPRri30cijQ+VW2MLUD+gShWPTrg2Jp9bPZD6fdxD6NH2IxhDI0C6RhZ6NML3EcE3JyFOoTgJcQrFSYhTKE5CnEJxEuIUipMQp5hWSr+Hp0Yfn+G+OJNoVF3vGFOjj97iXkBzi7i1f6+H0/mTif71bo0J1ZaVsrikt+EXEQlH9O8sIjI3OwtjyMLo9XDRwWSCn0vXsAesHkIJMJn73ds3cE84gu2eUgnbNr0+thzCEd1askZJiODJ1paVUm/Ujb+Jf1dbD/X+SOMxvo6yMUUbwTcnIU6hOAlxCsVJiFMoTkKcQnES4hSKkxCnmFbKzAzuE1Q32tXf29hQ1yeP8En/vdf7MLZ6bwvGqhVsi/T7eoVDLo8thdMTPNHYmoRcvsWVLiUwNdq6FmsMQtmYsB0E/ZtERDoJo2pioMdGRmXScIj/3v4bbMFEIvga54v6aIWth/g3cHqGn9n1Ne7tND01DWPn5xcwFgG2WSyG7bRQCH9nBN+chDiF4iTEKRQnIU6hOAlxCsVJiFMoTkKcYlop6xtrMLZqTL0+v9QrVlZX1+Gep7/5Dxi7PNPb34uIPNjEf7N6p9sRgQC2B0olPAn55cuXMNZu4zEOMVDxIYKrYPKG3VM1pmjfNfF1BEO4UdpooNtOVhOsvmEtZabwNHKL27JuSdXruILk6hrbHr0utnvyOXyPM8Yk6t093faz7LTlFTzWAsE3JyFOoTgJcQrFSYhTKE5CnEJxEuIUipMQp5hWitGvSD777DMY+9nnn6vrfeMPLq+vwtjzZ7+FsT/4id5sSURkbkafDn1yiptnBYwx2sViEcZmjSZeVtOw/82ecAjHQoZdEjD+F19d6tUb0TDekzBmx6wsY+vg0phtcnR4qK4vlHBlTyyGG41Zz7PTxb+DO2OKObqPzSZuYDccGmIC8M1JiFMoTkKcQnES4hSKkxCnUJyEOIXiJMQpZo7/K6NJ0/oGrgZ5tK2PkG8YlRtb97GV8rO/+ksYOzzAM1aSSX30uVVpETIsDMs6mJrGzaIseyMK5spYe9IZPMZ+vogth0YD2wPPnj5X18dDPGukMIurOj755BMYW1xchDFkU1iVRDFwD0VE5uex/bX3eg/GXr3ahbFcbk5dn53Vm5OJiEzG2NJB8M1JiFMoTkKcQnES4hSKkxCnUJyEOMXM1taqVRi7uUrB2Ac7O+p6vYWztYMu7hGzvHYPxr589gxfx+98W12fKWThnimj983btwcw1u/pPXhE7EPsjUZDXUeZZhGRfDYHY4kE7leEPksET/uuGKMfVtbw4fZmE2eGEwn83TY39Wd9eopHLpycHsNYyCgSGI9gSOo1fK/m5vRscyyOs8Z7uzj7i+CbkxCnUJyEOIXiJMQpFCchTqE4CXEKxUmIU0wrJWdM/o0Z04njIDaK414v5TY+6L3x8AMY++U//D2Mdau6PfDxRx/CPWiEg4jIcIAPgZ+fnMNYKIj/ByJ7KZ3Gh9tn53C/ouEI+wPjMb7+Ykk/IF4s6Ye8RUS+/4Pvw5hlpXQ6uNcOmnpdKuFD5ReX+N6j8Q4iIuEI/j2OjYPq7bb+3eaMPlK3N7hvEoJvTkKcQnES4hSKkxCnUJyEOIXiJMQpFCchTglMJhPcnIUQ8o3BNychTqE4CXEKxUmIUyhOQpxCcRLiFIqTEKf8D2226u0CosSZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root (1.00)\n",
      "├── insects (0.49)\n",
      "│   ├── beetle (0.49)\n",
      "├── large_carnivores (0.50)\n",
      "│   └── wolf (0.50)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confidence split between two independent branches\n",
    "two_branches = []\n",
    "for i, fitted_abstraction in enumerate(tqdm(fitted_abstractions)):\n",
    "    non_zero_level_1_nodes = get_nodes_at_level(fitted_abstraction, 1, True)\n",
    "    non_zero_leaf_nodes = get_nodes_at_level(fitted_abstraction, 2, True)\n",
    "    if len(non_zero_leaf_nodes) != 2 or len(non_zero_level_1_nodes) != 2:\n",
    "        continue\n",
    "    if 0.45 <= np.max([node.data for node in non_zero_leaf_nodes]) <= 0.55 and 0.45 <= np.max([node.data for node in non_zero_level_1_nodes]) <= 0.55:\n",
    "        two_branches.append(i)\n",
    "\n",
    "print(f'{len(two_branches)} images where the model is split between two branches through two superclass.')\n",
    "show_examples(two_branches, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87cdc1-0abf-4894-b993-50af55b59aeb",
   "metadata": {},
   "source": [
    "### Concept Co-Confusion\n",
    "We can also use abstraction alignment to indentify pairs of concepts that the model commonly confuses. These concepts can help us identify misalignments between human-similar concepts and model-similar concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50c98c61-1032-4a7a-a7b2-3f17ec21d3f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [13:48<00:00, 12.08it/s]\n"
     ]
    }
   ],
   "source": [
    "coconfusion = metrics.concept_coconfusion(fitted_abstractions, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f47fea93-3ae0-43f6-990d-a67125b83874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIGHEST OVERALL COCONFUSION:\n",
      "couch,household_furniture --- 30.58%\n",
      "household_electrical_devices,lamp --- 29.65%\n",
      "fruit_and_vegetables,mushroom --- 29.52%\n",
      "food_containers,household_electrical_devices --- 29.37%\n",
      "medium_mammals,small_mammals --- 29.00%\n"
     ]
    }
   ],
   "source": [
    "# Highest overall confusion\n",
    "coconfusion_non_root = {k:v for k, v in coconfusion.items() if 'root' not in k}\n",
    "sorted_pairs = [k for k, v in sorted(coconfusion.items(), key=lambda item: item[1], reverse=True)]\n",
    "print('HIGHEST OVERALL COCONFUSION:')\n",
    "for i in range(5):\n",
    "    print(f'{sorted_pairs[i]} --- {coconfusion[sorted_pairs[i]]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8277e6e-98a2-4f0c-9075-e3757a2c29ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIGHEST SUPERCLASS COCONFUSION:\n",
      "food_containers,household_electrical_devices --- 29.37%\n",
      "medium_mammals,small_mammals --- 29.00%\n",
      "non-insect_invertebrates,reptiles --- 28.76%\n",
      "vehicles_1,vehicles_2 --- 28.32%\n",
      "insects,non-insect_invertebrates --- 28.31%\n"
     ]
    }
   ],
   "source": [
    "# Confusion between superclass nodes\n",
    "superclass_pairs = [pair for pair in sorted_pairs if pair.split(',')[0] in cifar_metadata.SUPERCLASS_LABELS and pair.split(',')[1] in cifar_metadata.SUPERCLASS_LABELS]\n",
    "print('HIGHEST SUPERCLASS COCONFUSION:')\n",
    "for i in range(5):\n",
    "    print(f'{superclass_pairs[i]} --- {coconfusion[superclass_pairs[i]]:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31006a3d-d4ec-4cf6-95b8-551ddcada600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
