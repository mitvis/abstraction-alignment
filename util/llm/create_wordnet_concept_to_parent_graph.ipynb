{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1733bf6b-5b2b-4757-a1ac-3ee2bb0327f1",
   "metadata": {},
   "source": [
    "# Create a Human Abstraction Graph Relevant to the LLM Task\n",
    "To measure the alignment of LLMs, we create a human abstraction graph that represents their task. In the S-TEST dataset there are occupation (P106) and location (P131 and P19) tasks. To measure their alignment we map the words in the dataset to concepts in the WordNet graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5a21db-d30d-45e4-922d-881ee69e4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef2a3a0-9d00-4356-8cfe-70561601322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from queue import Queue\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '../../')))\n",
    "from graph import Graph, Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0c14a-5418-4297-b1fb-baf57bd9213b",
   "metadata": {},
   "source": [
    "## Load the S-TEST Specificity Testing Dataset\n",
    "The S-TEST dataset contains sentences that test the model on a subject's occupation, location, or place of brith. For instance occupation sentences are in the format \"Cher is a [MASK] by profession.\" Each sentence has a corresponding specific label (e.g., \"artist\"). Here we load the S-TEST data as well as the prediction results for 5 BERT, RoBERTa, and GPT-2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e392339-6b08-4273-86d4-95ff1310ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'S-TEST/data/S-TEST/'\n",
    "RESULTS_DIR = 'S-TEST/output/results/'\n",
    "\n",
    "MODELS = [\n",
    "    'bert_base', \n",
    "    'bert_large', \n",
    "    'roberta.base', \n",
    "    'roberta.large', \n",
    "    'gpt2',\n",
    "]\n",
    "TASKS = [\n",
    "    {'name': 'occupation', 'id': 'P106', 'up_fn': 'hypernyms', 'down_fn': 'hyponyms', 'root': wn.synset('person.n.01'), 'threshold': 0.01},\n",
    "    {'name': 'location', 'id': 'P131', 'up_fn': 'part_holonyms', 'down_fn': 'part_meronyms', 'root': None, 'threshold': 0.01},\n",
    "    {'name': 'place of birth', 'id': 'P19', 'up_fn': 'part_holonyms', 'down_fn': 'part_meronyms', 'root': None, 'threshold': 0.01},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d06833e-1d50-4d86-8d6a-37be237f354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_instances(task_id):\n",
    "    with open(os.path.join(DATA_DIR, f'{task_id}.jsonl'), 'r') as f:\n",
    "        data = [json.loads(l) for l in f]\n",
    "    instances = [(d['sub_label'], d['obj_label'], d['obj2_label']) for d in data]\n",
    "    return instances\n",
    "\n",
    "def load_data(task_id):\n",
    "    \"\"\"Load the data instances for a task_id. There can be duplicates, but we\n",
    "    handle them the same way the S-TEST repo does.\"\"\"\n",
    "    data = {}\n",
    "    with open(os.path.join(DATA_DIR, f'{task_id}.jsonl'), 'r') as f:\n",
    "        for line in f:\n",
    "            datum = json.loads(line)\n",
    "            data[datum['sub_label']] = datum\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4c9acf1-9b81-4e21-86ea-9c9ec5ccb5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 instances for occupation prediciton task.\n",
      "Example data:\n",
      "{'sub_uri': 'Q39074561', 'sub_label': 'Joe Carter', 'obj_uri': 'Q1371925', 'obj_label': 'announcer', 'obj_value': 2.0, 'obj2_uri': 'Q1930187', 'obj2_label': 'journalist', 'obj2_value': 3.0, 'predicate_id': 'P106'}\n"
     ]
    }
   ],
   "source": [
    "TASK = TASKS[0]\n",
    "MODEL = MODELS[0]\n",
    "\n",
    "DATA = load_data(TASK['id'])\n",
    "print(f\"{len(DATA)} instances for {TASK['name']} prediciton task.\")\n",
    "print(f\"Example data:\")\n",
    "print(DATA[list(DATA.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c390720-5f74-44c4-83dc-b0cc54201cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_results(model_name, task_id, results_dir = RESULTS_DIR):\n",
    "    with open(os.path.join(results_dir, model_name, task_id, 'result.pkl'), 'rb') as f:\n",
    "        results = pickle.load(f)['list_of_results']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e82c68d-4156-4bff-b9b0-c0d88e15e86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 predictions for bert_base on occupation prediciton task.\n",
      "Predictions for results[0] sum to 0.8970748439562531\n",
      "Computed probabilities for 18173 words.\n"
     ]
    }
   ],
   "source": [
    "RESULTS = load_model_results(MODEL, TASK['id'])\n",
    "print(f\"{len(RESULTS)} predictions for {MODEL} on {TASK['name']} prediciton task.\")\n",
    "print(f\"Predictions for results[0] sum to {np.sum([np.exp(w['log_prob']) for w in RESULTS[0]['masked_topk']['topk']])}\")\n",
    "print(f\"Computed probabilities for {len(RESULTS[0]['masked_topk']['topk'])} words.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6926f2d3-179e-4c28-8679-f92983da190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 5 instances with labels ['man', 'female', 'position', 'humour', 'man']\n",
      "Resulting dataset has 4995/5000 labels mapping to 4995 total/103 unique synsets\n",
      "First 5 labels: ['architect' 'philosopher' 'poet' 'mechanic' 'priest']\n",
      "First 5 synsets: [Synset('architect.n.01'), Synset('philosopher.n.01'), Synset('poet.n.01'), Synset('machinist.n.01'), Synset('priest.n.01')]\n"
     ]
    }
   ],
   "source": [
    "ALL_LABELS = [DATA[result['sample']['sub_label']]['obj_label'] for result in RESULTS]\n",
    "with open(os.path.join(DATA_DIR, f\"{TASK['id']}_synsets.json\"), 'r') as f:\n",
    "    label_synsets = json.load(f)\n",
    "    label_to_synset = {label: wn.synset(synset) for label, synset in label_synsets if synset is not None}\n",
    "IDX_TO_KEEP = [i for i in range(len(ALL_LABELS)) if ALL_LABELS[i] in label_to_synset]\n",
    "print(f'Removed {len(RESULTS) - len(IDX_TO_KEEP)} instances with labels {[label for i, label in enumerate(ALL_LABELS) if i not in IDX_TO_KEEP]}')\n",
    "\n",
    "LABELS = np.array(ALL_LABELS)[IDX_TO_KEEP]\n",
    "SYNSETS = [label_to_synset[label] for label in LABELS]\n",
    "print(f'Resulting dataset has {len(LABELS)}/{len(ALL_LABELS)} labels mapping to {len(SYNSETS)} total/{len(set(SYNSETS))} unique synsets')\n",
    "print(f'First 5 labels: {LABELS[:5]}')\n",
    "print(f'First 5 synsets: {SYNSETS[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a751b4-57ce-4de9-b52b-1e7440fe08f5",
   "metadata": {},
   "source": [
    "## Create the WordNet Human Abstraction Graph\n",
    "We use WordNet to represent the human abstraction graph for each task. For instance, for occupation prediction, we take the subset of WordNet that is connected to any of the labels in the S-TEST dataset. This creates a human abstraction of occupations ranging from low-level labels (e.g., \"poet\") to high-level profession concepts (e.g., \"communicator\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56f3f4-89c6-4d02-8cc8-157d2d6ab04d",
   "metadata": {},
   "source": [
    "#### First, get all the WordNet concepts that are related to any task label in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3adca7c8-7b73-4402-9748-3b8571d4aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566 concepts synsets related to 103 synsets from the occupation prediction task.\n",
      "Example concepts: ['bibliotist', 'letterman', 'assassin', 'kerb_crawler', 'paster']\n"
     ]
    }
   ],
   "source": [
    "def get_synset_name(synset):\n",
    "    return synset.name().split('.')[0]\n",
    "\n",
    "def get_synsets_relatives(synset, traversal_fn_name, root=None, include_self=True):\n",
    "    traversal_fn = getattr(synset, traversal_fn_name)\n",
    "    words = set([])\n",
    "    if include_self:\n",
    "        words.add(get_synset_name(synset))\n",
    "    if (root is not None and synset == root) or len(traversal_fn()) == 0:\n",
    "        return words\n",
    "    for word in traversal_fn():\n",
    "        next_words = get_synsets_relatives(word, traversal_fn_name, root)\n",
    "        words.update(next_words)\n",
    "    return words\n",
    "\n",
    "def get_task_synsets(task, synset_labels):\n",
    "    task_synsets = set([])\n",
    "    for synset in synset_labels:\n",
    "        if synset in task_synsets: \n",
    "            continue\n",
    "        children = get_synsets_relatives(synset, task['down_fn'], root=task['root'], include_self=False)\n",
    "        parents = get_synsets_relatives(synset, task['up_fn'], root=task['root'], include_self=False)\n",
    "        task_synsets.add(get_synset_name(synset))\n",
    "        task_synsets.update(children)\n",
    "        task_synsets.update(parents)\n",
    "    return task_synsets\n",
    "\n",
    "task_synsets = get_task_synsets(TASK, SYNSETS)\n",
    "print(f\"{len(task_synsets)} concepts synsets related to {len(set(SYNSETS))} synsets from the {TASK['name']} prediction task.\")\n",
    "print(f'Example concepts: {random.sample(task_synsets, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e60633-ff8c-41b0-9664-af83bd9f23df",
   "metadata": {},
   "source": [
    "#### Next, create a DAG representing all of the concepts and their relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaa6c712-6bae-4daa-b405-e7b94abf1569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created WordNet DAG with root node 'person' and 1527 synset concepts.\n"
     ]
    }
   ],
   "source": [
    "def create_wordnet_dag(task, task_synsets):\n",
    "    root = task['root']\n",
    "    nodes = {}\n",
    "    queue = Queue()\n",
    "    queue.put((root, None)) # Queue contains the synset and its parent DAG node.\n",
    "    while not queue.empty():\n",
    "        synset, parent_node = queue.get()\n",
    "        synset_name = get_synset_name(synset)\n",
    "        \n",
    "        # Create a node for the synset if it does not already exit.\n",
    "        if synset_name not in nodes:\n",
    "            synset_node = Node(synset_name)\n",
    "        else: \n",
    "            synset_node = nodes[synset_name]\n",
    "            \n",
    "        # Connect the node to its parent and update the graph.\n",
    "        if parent_node is not None: # Only the root node has parent = None.\n",
    "            parent_node.connect_child(synset_node)\n",
    "        nodes[synset_name] = synset_node\n",
    "        \n",
    "        # Continue the traversal down the graph.\n",
    "        traversal_fn = getattr(synset, task['down_fn'])\n",
    "        for next_synset in traversal_fn():\n",
    "            if get_synset_name(next_synset) not in task_synsets:\n",
    "                # print(get_synset_name(next_synset))\n",
    "                continue # Skip relatives that are not related to the task.\n",
    "            queue.put((next_synset, synset_node))\n",
    "    wordnet_dag = Graph(nodes, get_synset_name(root))\n",
    "    wordnet_dag.finalize()\n",
    "    return wordnet_dag\n",
    "            \n",
    "wordnet_dag = create_wordnet_dag(TASK, task_synsets)\n",
    "print(f\"Created WordNet DAG with root node '{wordnet_dag.root_id}' and {len(wordnet_dag.nodes)} synset concepts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb7ef1-1ce3-48a9-969e-17ef42cb30f9",
   "metadata": {},
   "source": [
    "#### Finally, map concepts in the WordNet DAG to their most accurate parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c4974a-c834-49b0-8548-9eb44ea872af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parents per node -- Counter({1: 1380, 2: 133, 3: 13, 0: 1})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parents per node -- {Counter([len(node.parents) for node in wordnet_dag.nodes.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "190df535-da31-4515-9ecb-7eb7391b9559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded node to parent mapping for 146 nodes.\n"
     ]
    }
   ],
   "source": [
    "# Manually select the most relevant parent for each concept node\n",
    "name_to_parent_filename = os.path.join(RESULTS_DIR, 'name_to_parent.json')\n",
    "if os.path.isfile(name_to_parent_filename):\n",
    "    with open(name_to_parent_filename, 'r') as f:\n",
    "        node_name_to_parent_name = json.load(f)\n",
    "    print(f\"Loaded node to parent mapping for {len(node_name_to_parent_name)} nodes.\")\n",
    "else:\n",
    "    node_name_to_parent_name = {}\n",
    "    for node_name, node in wordnet_dag.nodes.items():\n",
    "        if node_name in node_name_to_parent_name: continue\n",
    "        if len(node.parents) <= 1: continue\n",
    "        non_root_parents = [parent for parent in node.parents if parent.name != 'person']\n",
    "        if len(non_root_parents) == 1:\n",
    "            node_name_to_parent_name[node_name] = non_root_parents[0].name\n",
    "        else:\n",
    "            parents = [(parent.name, parent.depth) for parent in node.parents]\n",
    "            parents.sort(key = lambda x: x[1], reverse=True)\n",
    "            selected_parent = ''\n",
    "            while selected_parent not in wordnet_dag.nodes:\n",
    "                print(f\"Parent options for {node_name} are {parents}. Children are {node.children}.\")\n",
    "                selected_parent = input()\n",
    "            node_name_to_parent_name[node_name] = selected_parent\n",
    "    print(f\"Loaded node to parent mapping for {len(node_name_to_parent_name)} nodes.\")\n",
    "    with open(name_to_parent_filename, 'w') as f:\n",
    "        json.dump(node_name_to_parent_name, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c09b7-3a5f-4ba3-8a49-1f9872c27638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
